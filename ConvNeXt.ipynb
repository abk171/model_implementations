{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ade211c3-c527-43bc-8e4d-dda0562d9dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b0d5377-a460-46b9-87ad-f6932d6282a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myLayerNorm(nn.Module):\n",
    "  def __init__(self, dims, eps=1e-6):\n",
    "    super().__init__()\n",
    "    self.norm = nn.LayerNorm(dims, eps)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x.permute(0, 2, 3, 1)\n",
    "    x = self.norm(x)\n",
    "    x = x.permute(0, 3, 1, 2)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daaae54d-f690-48a0-85f0-adf949bbd155",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNeXtBlock2D(nn.Module):\n",
    "  def __init__(self, dim, layer_scale_init_value=1e-6, drop=0.2):\n",
    "    super().__init__()\n",
    "\n",
    "    self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim)  # depthwise conv\n",
    "\n",
    "    self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    self.pwconv1 = nn.Linear(dim, 4 * dim)\n",
    "    self.act = nn.GELU()\n",
    "    self.pwconv2 = nn.Linear(4 * dim, dim)\n",
    "\n",
    "    self.dropout = nn.Dropout2d(p=drop)\n",
    "\n",
    "    self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim,)), requires_grad=True) if layer_scale_init_value > 0 else None\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    residual = x\n",
    "    x = self.dwconv(x)\n",
    "\n",
    "    # Transpose for LayerNorm\n",
    "    x = x.permute(0, 2, 3, 1)\n",
    "    x = self.norm(x)\n",
    "\n",
    "    x = self.pwconv1(x)\n",
    "    x = self.act(x)\n",
    "    x = self.pwconv2(x)\n",
    "\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    if self.gamma is not None:\n",
    "        x = self.gamma * x\n",
    "\n",
    "    # Transpose back to (B, C, H, W)\n",
    "    x = x.permute(0, 3, 1, 2)\n",
    "    # no drop path y\n",
    "    return residual + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bc73932-0356-4583-95e4-45c5bf5fcca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 96, 224, 224]           4,800\n",
      "         LayerNorm-2         [-1, 224, 224, 96]             192\n",
      "            Linear-3        [-1, 224, 224, 384]          37,248\n",
      "              GELU-4        [-1, 224, 224, 384]               0\n",
      "            Linear-5         [-1, 224, 224, 96]          36,960\n",
      "         Dropout2d-6         [-1, 224, 224, 96]               0\n",
      "================================================================\n",
      "Total params: 79,200\n",
      "Trainable params: 79,200\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 18.38\n",
      "Forward/backward pass size (MB): 441.00\n",
      "Params size (MB): 0.30\n",
      "Estimated Total Size (MB): 459.68\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(ConvNeXtBlock2D(96), (96,224,224), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86ce696a-b108-4ffd-88d6-47598d8d1271",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNext(nn.Module):\n",
    "    def __init__(self, in_chans=1, dims=[32, 64, 128, 256], stages=[1, 1, 3, 1]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.dims = dims\n",
    "        self.stages = stages\n",
    "\n",
    "        self.downsample_layers = nn.ModuleList()  # stem and 3 intermediate downsampling conv layers\n",
    "        stem = nn.Sequential(\n",
    "            nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4),\n",
    "            myLayerNorm(dims[0], eps=1e-6)\n",
    "        )\n",
    "        self.downsample_layers.append(stem)\n",
    "        for i in range(3):\n",
    "            downsample_layer = nn.Sequential(\n",
    "                myLayerNorm(dims[i], eps=1e-6),\n",
    "                nn.Conv2d(dims[i], dims[i + 1], kernel_size=2, stride=2),\n",
    "            )\n",
    "            self.downsample_layers.append(downsample_layer)\n",
    "\n",
    "        self.model_layers = nn.ModuleList()\n",
    "        for i, stage_length in enumerate(stages):\n",
    "            stage = nn.ModuleList([ConvNeXtBlock2D(dims[i]) for _ in range(stage_length)])\n",
    "            self.model_layers.append(stage)\n",
    "\n",
    "        self.pooling = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.final_norm = nn.LayerNorm(dims[-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.dims)):\n",
    "            x = self.downsample_layers[i](x)\n",
    "            for layer in self.model_layers[i]:\n",
    "                x = layer(x)\n",
    "\n",
    "        x = self.pooling(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.final_norm(x)  # Final normalization\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd4505f7-9f85-4860-8170-570144c8ce9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 96, 56, 56]           4,704\n",
      "         LayerNorm-2           [-1, 56, 56, 96]             192\n",
      "       myLayerNorm-3           [-1, 96, 56, 56]               0\n",
      "            Conv2d-4           [-1, 96, 56, 56]           4,800\n",
      "         LayerNorm-5           [-1, 56, 56, 96]             192\n",
      "            Linear-6          [-1, 56, 56, 384]          37,248\n",
      "              GELU-7          [-1, 56, 56, 384]               0\n",
      "            Linear-8           [-1, 56, 56, 96]          36,960\n",
      "         Dropout2d-9           [-1, 56, 56, 96]               0\n",
      "  ConvNeXtBlock2D-10           [-1, 96, 56, 56]               0\n",
      "           Conv2d-11           [-1, 96, 56, 56]           4,800\n",
      "        LayerNorm-12           [-1, 56, 56, 96]             192\n",
      "           Linear-13          [-1, 56, 56, 384]          37,248\n",
      "             GELU-14          [-1, 56, 56, 384]               0\n",
      "           Linear-15           [-1, 56, 56, 96]          36,960\n",
      "        Dropout2d-16           [-1, 56, 56, 96]               0\n",
      "  ConvNeXtBlock2D-17           [-1, 96, 56, 56]               0\n",
      "           Conv2d-18           [-1, 96, 56, 56]           4,800\n",
      "        LayerNorm-19           [-1, 56, 56, 96]             192\n",
      "           Linear-20          [-1, 56, 56, 384]          37,248\n",
      "             GELU-21          [-1, 56, 56, 384]               0\n",
      "           Linear-22           [-1, 56, 56, 96]          36,960\n",
      "        Dropout2d-23           [-1, 56, 56, 96]               0\n",
      "  ConvNeXtBlock2D-24           [-1, 96, 56, 56]               0\n",
      "        LayerNorm-25           [-1, 56, 56, 96]             192\n",
      "      myLayerNorm-26           [-1, 96, 56, 56]               0\n",
      "           Conv2d-27          [-1, 192, 28, 28]          73,920\n",
      "           Conv2d-28          [-1, 192, 28, 28]           9,600\n",
      "        LayerNorm-29          [-1, 28, 28, 192]             384\n",
      "           Linear-30          [-1, 28, 28, 768]         148,224\n",
      "             GELU-31          [-1, 28, 28, 768]               0\n",
      "           Linear-32          [-1, 28, 28, 192]         147,648\n",
      "        Dropout2d-33          [-1, 28, 28, 192]               0\n",
      "  ConvNeXtBlock2D-34          [-1, 192, 28, 28]               0\n",
      "           Conv2d-35          [-1, 192, 28, 28]           9,600\n",
      "        LayerNorm-36          [-1, 28, 28, 192]             384\n",
      "           Linear-37          [-1, 28, 28, 768]         148,224\n",
      "             GELU-38          [-1, 28, 28, 768]               0\n",
      "           Linear-39          [-1, 28, 28, 192]         147,648\n",
      "        Dropout2d-40          [-1, 28, 28, 192]               0\n",
      "  ConvNeXtBlock2D-41          [-1, 192, 28, 28]               0\n",
      "           Conv2d-42          [-1, 192, 28, 28]           9,600\n",
      "        LayerNorm-43          [-1, 28, 28, 192]             384\n",
      "           Linear-44          [-1, 28, 28, 768]         148,224\n",
      "             GELU-45          [-1, 28, 28, 768]               0\n",
      "           Linear-46          [-1, 28, 28, 192]         147,648\n",
      "        Dropout2d-47          [-1, 28, 28, 192]               0\n",
      "  ConvNeXtBlock2D-48          [-1, 192, 28, 28]               0\n",
      "        LayerNorm-49          [-1, 28, 28, 192]             384\n",
      "      myLayerNorm-50          [-1, 192, 28, 28]               0\n",
      "           Conv2d-51          [-1, 384, 14, 14]         295,296\n",
      "           Conv2d-52          [-1, 384, 14, 14]          19,200\n",
      "        LayerNorm-53          [-1, 14, 14, 384]             768\n",
      "           Linear-54         [-1, 14, 14, 1536]         591,360\n",
      "             GELU-55         [-1, 14, 14, 1536]               0\n",
      "           Linear-56          [-1, 14, 14, 384]         590,208\n",
      "        Dropout2d-57          [-1, 14, 14, 384]               0\n",
      "  ConvNeXtBlock2D-58          [-1, 384, 14, 14]               0\n",
      "           Conv2d-59          [-1, 384, 14, 14]          19,200\n",
      "        LayerNorm-60          [-1, 14, 14, 384]             768\n",
      "           Linear-61         [-1, 14, 14, 1536]         591,360\n",
      "             GELU-62         [-1, 14, 14, 1536]               0\n",
      "           Linear-63          [-1, 14, 14, 384]         590,208\n",
      "        Dropout2d-64          [-1, 14, 14, 384]               0\n",
      "  ConvNeXtBlock2D-65          [-1, 384, 14, 14]               0\n",
      "           Conv2d-66          [-1, 384, 14, 14]          19,200\n",
      "        LayerNorm-67          [-1, 14, 14, 384]             768\n",
      "           Linear-68         [-1, 14, 14, 1536]         591,360\n",
      "             GELU-69         [-1, 14, 14, 1536]               0\n",
      "           Linear-70          [-1, 14, 14, 384]         590,208\n",
      "        Dropout2d-71          [-1, 14, 14, 384]               0\n",
      "  ConvNeXtBlock2D-72          [-1, 384, 14, 14]               0\n",
      "           Conv2d-73          [-1, 384, 14, 14]          19,200\n",
      "        LayerNorm-74          [-1, 14, 14, 384]             768\n",
      "           Linear-75         [-1, 14, 14, 1536]         591,360\n",
      "             GELU-76         [-1, 14, 14, 1536]               0\n",
      "           Linear-77          [-1, 14, 14, 384]         590,208\n",
      "        Dropout2d-78          [-1, 14, 14, 384]               0\n",
      "  ConvNeXtBlock2D-79          [-1, 384, 14, 14]               0\n",
      "           Conv2d-80          [-1, 384, 14, 14]          19,200\n",
      "        LayerNorm-81          [-1, 14, 14, 384]             768\n",
      "           Linear-82         [-1, 14, 14, 1536]         591,360\n",
      "             GELU-83         [-1, 14, 14, 1536]               0\n",
      "           Linear-84          [-1, 14, 14, 384]         590,208\n",
      "        Dropout2d-85          [-1, 14, 14, 384]               0\n",
      "  ConvNeXtBlock2D-86          [-1, 384, 14, 14]               0\n",
      "           Conv2d-87          [-1, 384, 14, 14]          19,200\n",
      "        LayerNorm-88          [-1, 14, 14, 384]             768\n",
      "           Linear-89         [-1, 14, 14, 1536]         591,360\n",
      "             GELU-90         [-1, 14, 14, 1536]               0\n",
      "           Linear-91          [-1, 14, 14, 384]         590,208\n",
      "        Dropout2d-92          [-1, 14, 14, 384]               0\n",
      "  ConvNeXtBlock2D-93          [-1, 384, 14, 14]               0\n",
      "           Conv2d-94          [-1, 384, 14, 14]          19,200\n",
      "        LayerNorm-95          [-1, 14, 14, 384]             768\n",
      "           Linear-96         [-1, 14, 14, 1536]         591,360\n",
      "             GELU-97         [-1, 14, 14, 1536]               0\n",
      "           Linear-98          [-1, 14, 14, 384]         590,208\n",
      "        Dropout2d-99          [-1, 14, 14, 384]               0\n",
      " ConvNeXtBlock2D-100          [-1, 384, 14, 14]               0\n",
      "          Conv2d-101          [-1, 384, 14, 14]          19,200\n",
      "       LayerNorm-102          [-1, 14, 14, 384]             768\n",
      "          Linear-103         [-1, 14, 14, 1536]         591,360\n",
      "            GELU-104         [-1, 14, 14, 1536]               0\n",
      "          Linear-105          [-1, 14, 14, 384]         590,208\n",
      "       Dropout2d-106          [-1, 14, 14, 384]               0\n",
      " ConvNeXtBlock2D-107          [-1, 384, 14, 14]               0\n",
      "          Conv2d-108          [-1, 384, 14, 14]          19,200\n",
      "       LayerNorm-109          [-1, 14, 14, 384]             768\n",
      "          Linear-110         [-1, 14, 14, 1536]         591,360\n",
      "            GELU-111         [-1, 14, 14, 1536]               0\n",
      "          Linear-112          [-1, 14, 14, 384]         590,208\n",
      "       Dropout2d-113          [-1, 14, 14, 384]               0\n",
      " ConvNeXtBlock2D-114          [-1, 384, 14, 14]               0\n",
      "       LayerNorm-115          [-1, 14, 14, 384]             768\n",
      "     myLayerNorm-116          [-1, 384, 14, 14]               0\n",
      "          Conv2d-117            [-1, 768, 7, 7]       1,180,416\n",
      "          Conv2d-118            [-1, 768, 7, 7]          38,400\n",
      "       LayerNorm-119            [-1, 7, 7, 768]           1,536\n",
      "          Linear-120           [-1, 7, 7, 3072]       2,362,368\n",
      "            GELU-121           [-1, 7, 7, 3072]               0\n",
      "          Linear-122            [-1, 7, 7, 768]       2,360,064\n",
      "       Dropout2d-123            [-1, 7, 7, 768]               0\n",
      " ConvNeXtBlock2D-124            [-1, 768, 7, 7]               0\n",
      "          Conv2d-125            [-1, 768, 7, 7]          38,400\n",
      "       LayerNorm-126            [-1, 7, 7, 768]           1,536\n",
      "          Linear-127           [-1, 7, 7, 3072]       2,362,368\n",
      "            GELU-128           [-1, 7, 7, 3072]               0\n",
      "          Linear-129            [-1, 7, 7, 768]       2,360,064\n",
      "       Dropout2d-130            [-1, 7, 7, 768]               0\n",
      " ConvNeXtBlock2D-131            [-1, 768, 7, 7]               0\n",
      "          Conv2d-132            [-1, 768, 7, 7]          38,400\n",
      "       LayerNorm-133            [-1, 7, 7, 768]           1,536\n",
      "          Linear-134           [-1, 7, 7, 3072]       2,362,368\n",
      "            GELU-135           [-1, 7, 7, 3072]               0\n",
      "          Linear-136            [-1, 7, 7, 768]       2,360,064\n",
      "       Dropout2d-137            [-1, 7, 7, 768]               0\n",
      " ConvNeXtBlock2D-138            [-1, 768, 7, 7]               0\n",
      "AdaptiveAvgPool2d-139            [-1, 768, 1, 1]               0\n",
      "         Flatten-140                  [-1, 768]               0\n",
      "       LayerNorm-141                  [-1, 768]           1,536\n",
      "================================================================\n",
      "Total params: 27,813,504\n",
      "Trainable params: 27,813,504\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 229.71\n",
      "Params size (MB): 106.10\n",
      "Estimated Total Size (MB): 336.38\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(ConvNext(in_chans=3,dims=[96,192,384,768],stages=[3,3,9,3]), (3,224,224), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be58c570-aaf7-410d-a073-eb1d5bd85507",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
